{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shitiza\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\shitiza\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "C:\\Users\\shitiza\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "import re,os,csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "import dedupe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Prcoess & Clean-up Data Sets before entity Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_objCols(dfx):\n",
    "    for column in dfx.columns.to_list() :\n",
    "        if np.dtype('O') == dfx[column].dtype:\n",
    "            #print(f'INFO: clean-up col : {column}')\n",
    "            dfx[column] = (dfx[column]\n",
    "                .str.replace(r'\\[|\\]-|,|;|;|\"|\\'','', regex=True,flags=re.IGNORECASE)\n",
    "                .str.replace(r'\\s+|/',' ', regex=True,flags=re.IGNORECASE)\n",
    "                .str.strip()\n",
    "                .str.strip('\"')\n",
    "                .str.strip(\"'\")\n",
    "                .str.strip('\\[')\n",
    "                .str.strip('\\]')\n",
    "                .str.lower()\n",
    "                .str.strip()\n",
    "                )\n",
    "    return dfx\n",
    "\n",
    "def cleanup_cols(dfx,outF,maxL=1000):\n",
    "    return (\n",
    "    clean_objCols(dfx)\n",
    "    #.assign(id = lambda df : df.id.astype(int))\n",
    "    .assign(id = lambda df : df.index)\n",
    "    .assign(fileName  = lambda df : [f + '__' + str(i) for f,i in zip(df.fileName,df.index)])\n",
    "    .loc[:maxL,['fileName','id','Title','Keywords']]\n",
    "    .set_index('fileName')\n",
    "    .fillna('')\n",
    "    .to_csv(outF)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read left & right datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'CONSOLIDATED_FOR_ENTITY_MATCHING'\n",
    "   \n",
    "OUT_DIR = 'ENTITY_MATCH_OUTPUT/pubMED_vs_clinicTrial'\n",
    "if not os.path.exists(OUT_DIR):\n",
    "    os.makedirs(OUT_DIR)\n",
    "\n",
    "\n",
    "o_left_file = DATA_DIR + '/all_pubmed_keyWords_FINAL.csv'\n",
    "o_right_file = DATA_DIR + '/all_clinictrial_keyWords_FINAL.csv'\n",
    "\n",
    "left_file = OUT_DIR + '/pubMedF.csv'\n",
    "right_file = OUT_DIR + '/clinicTrialF.csv'\n",
    "cleanup_cols(pd.read_csv(o_left_file).assign(fileName = 'pubMedF').astype(str),left_file)\n",
    "cleanup_cols(pd.read_csv(o_right_file).assign(fileName = 'clinicTrialF').astype(str),right_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Keywords</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fileName</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pubmedf__0</th>\n",
       "      <td>0</td>\n",
       "      <td>micra® leadless pacemaker</td>\n",
       "      <td>leadless presenting approach pacemaker datum ®...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pubmedf__1</th>\n",
       "      <td>1</td>\n",
       "      <td>pacemaker complications and costs: a nationwid...</td>\n",
       "      <td>leadless study novel may system datum pmsi sys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pubmedf__2</th>\n",
       "      <td>2</td>\n",
       "      <td>runaway pacemaker</td>\n",
       "      <td>examination datum implantation revealed excell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pubmedf__3</th>\n",
       "      <td>3</td>\n",
       "      <td>pacemaker reprogramming rarely needed after de...</td>\n",
       "      <td>action depending needed involved subsequent 17...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                              Title  \\\n",
       "fileName                                                            \n",
       "pubmedf__0   0                          micra® leadless pacemaker   \n",
       "pubmedf__1   1  pacemaker complications and costs: a nationwid...   \n",
       "pubmedf__2   2                                  runaway pacemaker   \n",
       "pubmedf__3   3  pacemaker reprogramming rarely needed after de...   \n",
       "\n",
       "                                                     Keywords  \n",
       "fileName                                                       \n",
       "pubmedf__0  leadless presenting approach pacemaker datum ®...  \n",
       "pubmedf__1  leadless study novel may system datum pmsi sys...  \n",
       "pubmedf__2  examination datum implantation revealed excell...  \n",
       "pubmedf__3  action depending needed involved subsequent 17...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pubMed_df = pd.read_csv(left_file).set_index('fileName') \n",
    "pubMed_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Keywords</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fileName</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>clinictrialf__0</th>\n",
       "      <td>0</td>\n",
       "      <td>taiwan registry for leadless pacemaker</td>\n",
       "      <td>leadless intervention pacemaker cardiac patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clinictrialf__1</th>\n",
       "      <td>1</td>\n",
       "      <td>placement of cardiac pacemaker trial (pocket)-rct</td>\n",
       "      <td>defibrillator pacemaker 691 totally child ). v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clinictrialf__2</th>\n",
       "      <td>2</td>\n",
       "      <td>cost-utility analysis of ambulatory care compa...</td>\n",
       "      <td>less hours old living 24 aortic patient arm ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clinictrialf__3</th>\n",
       "      <td>3</td>\n",
       "      <td>morbidity mortality and gender differences in ...</td>\n",
       "      <td>intervention pacemaker system get criteriather...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                              Title  \\\n",
       "fileName                                                                 \n",
       "clinictrialf__0   0             taiwan registry for leadless pacemaker   \n",
       "clinictrialf__1   1  placement of cardiac pacemaker trial (pocket)-rct   \n",
       "clinictrialf__2   2  cost-utility analysis of ambulatory care compa...   \n",
       "clinictrialf__3   3  morbidity mortality and gender differences in ...   \n",
       "\n",
       "                                                          Keywords  \n",
       "fileName                                                            \n",
       "clinictrialf__0  leadless intervention pacemaker cardiac patien...  \n",
       "clinictrialf__1  defibrillator pacemaker 691 totally child ). v...  \n",
       "clinictrialf__2  less hours old living 24 aortic patient arm ou...  \n",
       "clinictrialf__3  intervention pacemaker system get criteriather...  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clinic_df = pd.read_csv(right_file).set_index('fileName')\n",
    "clinic_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = pubMed_df.to_dict('index')\n",
    "data_2 = clinic_df.to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'Title': 'micra® leadless pacemaker',\n",
       " 'Keywords': 'leadless presenting approach pacemaker datum ® atrial similar traditional pacemakercardiac method therapy technology without miniature expectancy risk providesmicra invasive less high conclusion conventional transvenous crt versus year fibrillation therefore micra patients venous pacemakers device life'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1['pubmedf__0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'Title': 'taiwan registry for leadless pacemaker',\n",
       " 'Keywords': 'leadless intervention pacemaker cardiac patient patients 1therapy received taiwan child efficiency safety pacemaker exclusion procedure pilot 2 registrytaiwan specifically taiwan receive population selection tavi n attempt mri benefit surgical implantation world performance pacemakersuccessfully inclusion criteria pacemakers early registry'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2['clinictrialf__0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = OUT_DIR + '/data_matching_output.csv'\n",
    "settings_file = OUT_DIR + '/data_matching_learned_settings'\n",
    "training_file = OUT_DIR + '/data_matching_training_f.json'\n",
    "def descriptions():\n",
    "        for dataset in (data_1, data_2):\n",
    "            for record in dataset.values():\n",
    "                yield record['Title']   \n",
    "def brands():\n",
    "        for dataset in (data_1, data_2):\n",
    "            for record in dataset.values():\n",
    "                yield record['Keywords']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading from ENTITY_MATCH_OUTPUT/data_matching_learned_settings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.api:Predicate set:\n",
      "INFO:dedupe.api:(SimplePredicate: (sameSevenCharStartPredicate, Title), TfidfNGramSearchPredicate: (0.6, Title), TfidfTextSearchPredicate: (0.2, Title))\n",
      "INFO:dedupe.api:(SimplePredicate: (commonThreeTokens, Title), TfidfTextSearchPredicate: (0.2, Title), TfidfNGramSearchPredicate: (0.6, Title))\n",
      "INFO:dedupe.api:(SimplePredicate: (firstTwoTokensPredicate, Title), SimplePredicate: (suffixArray, Title), TfidfTextSearchPredicate: (0.2, Title))\n",
      "INFO:dedupe.api:SimplePredicate: (commonTwoTokens, Keywords)\n"
     ]
    }
   ],
   "source": [
    "    if os.path.exists(settings_file):\n",
    "        print('reading from', settings_file)\n",
    "        with open(settings_file, 'rb') as sf:\n",
    "            linker = dedupe.StaticRecordLink(sf)\n",
    "\n",
    "    else:\n",
    "        # Define the fields the linker will pay attention to\n",
    "        #\n",
    "        # Notice how we are telling the linker to use a custom field comparator\n",
    "        # for the 'price' field.\n",
    "        fields = [\n",
    "            {'field': 'Title', 'type': 'String'},\n",
    "            {'field': 'Title', 'type': 'Text', 'corpus': descriptions()},\n",
    "            {'field': 'Keywords', 'type': 'Text',\n",
    "             'has missing': True, 'corpus': brands()}\n",
    "            ]\n",
    "\n",
    "        # Create a new linker object and pass our data model to it.\n",
    "        linker = dedupe.RecordLink(fields)\n",
    "\n",
    "        # If we have training data saved from a previous run of linker,\n",
    "        # look for it an load it in.\n",
    "        # __Note:__ if you want to train from scratch, delete the training_file\n",
    "        if os.path.exists(training_file):\n",
    "            print('reading labeled examples from ', training_file)\n",
    "            with open(training_file) as tf:\n",
    "                linker.prepare_training(data_1,\n",
    "                                        data_2,\n",
    "                                        training_file=tf,\n",
    "                                        sample_size=15000)\n",
    "        else:\n",
    "            linker.prepare_training(data_1, data_2, sample_size=15000)\n",
    "\n",
    "        # ## Active learning\n",
    "        # Dedupe will find the next pair of records\n",
    "        # it is least certain about and ask you to label them as matches\n",
    "        # or not.\n",
    "        # use 'y', 'n' and 'u' keys to flag duplicates\n",
    "        # press 'f' when you are finished\n",
    "        print('starting active labeling...')\n",
    "\n",
    "        dedupe.console_label(linker)\n",
    "\n",
    "        linker.train()\n",
    "\n",
    "        # When finished, save our training away to disk\n",
    "        with open(training_file, 'w') as tf:\n",
    "            linker.write_training(tf)\n",
    "\n",
    "        # Save our weights and predicates to disk.  If the settings file\n",
    "        # exists, we will skip all the training and learning next time we run\n",
    "        # this file.\n",
    "        with open(settings_file, 'wb') as sf:\n",
    "            linker.write_settings(sf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering...\n",
      "# duplicate sets 906\n"
     ]
    }
   ],
   "source": [
    "    # ## Blocking\n",
    "\n",
    "    # ## Clustering\n",
    "\n",
    "    # Find the threshold that will maximize a weighted average of our\n",
    "    # precision and recall.  When we set the recall weight to 2, we are\n",
    "    # saying we care twice as much about recall as we do precision.\n",
    "    #\n",
    "    # If we had more data, we would not pass in all the blocked data into\n",
    "    # this function but a representative sample.\n",
    "\n",
    "    print('clustering...')\n",
    "    linked_records = linker.join(data_1, data_2, 0.0,'one-to-one')\n",
    "\n",
    "    print('# duplicate sets', len(linked_records))\n",
    "    # ## Writing Results\n",
    "\n",
    "    # Write our original data back out to a CSV with a new column called\n",
    "    # 'Cluster ID' which indicates which records refer to each other.\n",
    "\n",
    "    cluster_membership = {}\n",
    "    for cluster_id, (cluster, score) in enumerate(linked_records):\n",
    "        for record_id in cluster:\n",
    "            cluster_membership[record_id] = {'Cluster ID': cluster_id,\n",
    "                                             'Link Score': score}\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf8') as f:\n",
    "\n",
    "        header_unwritten = True\n",
    "\n",
    "        for fileno, filename in enumerate((left_file, right_file)):\n",
    "            with open(filename, 'r', encoding=\"utf8\") as f_input:\n",
    "                reader = csv.DictReader(f_input)\n",
    "\n",
    "                if header_unwritten:\n",
    "\n",
    "                    fieldnames = (['Cluster ID', 'Link Score', 'source file'] +\n",
    "                                  reader.fieldnames)\n",
    "\n",
    "                    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "                    writer.writeheader()\n",
    "\n",
    "                    header_unwritten = False\n",
    "\n",
    "                for row_id, row in enumerate(reader):\n",
    "\n",
    "                    record_id = str(re.sub(OUT_DIR,'',re.sub('\\.\\/|F\\.csv','',os.path.basename(filename).lower()))).replace('.csv','') + '__' + str(row_id)\n",
    "                    #print(record_id)\n",
    "                              \n",
    "                    cluster_details = cluster_membership.get(record_id, {})\n",
    "                    row['source file'] = fileno\n",
    "                    row.update(cluster_details)\n",
    "                   \n",
    "\n",
    "                    writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check to ensure Cluster-IN & Score in final matched output csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cluster ID</th>\n",
       "      <th>Link Score</th>\n",
       "      <th>source file</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1812.000000</td>\n",
       "      <td>1812.000000</td>\n",
       "      <td>2002.000000</td>\n",
       "      <td>2002.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>452.500000</td>\n",
       "      <td>0.284364</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>261.611711</td>\n",
       "      <td>0.188868</td>\n",
       "      <td>0.500125</td>\n",
       "      <td>289.035861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>226.000000</td>\n",
       "      <td>0.130286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>250.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>452.500000</td>\n",
       "      <td>0.233936</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>679.000000</td>\n",
       "      <td>0.415492</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>750.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>905.000000</td>\n",
       "      <td>0.928825</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Cluster ID   Link Score  source file           id\n",
       "count  1812.000000  1812.000000  2002.000000  2002.000000\n",
       "mean    452.500000     0.284364     0.500000   500.000000\n",
       "std     261.611711     0.188868     0.500125   289.035861\n",
       "min       0.000000     0.027700     0.000000     0.000000\n",
       "25%     226.000000     0.130286     0.000000   250.000000\n",
       "50%     452.500000     0.233936     0.500000   500.000000\n",
       "75%     679.000000     0.415492     1.000000   750.000000\n",
       "max     905.000000     0.928825     1.000000  1000.000000"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chk_df = pd.read_csv(output_file)\n",
    "len(chk_df['Cluster ID'].unique())\n",
    "g_m = chk_df['Cluster ID'] != np.nan\n",
    "chk_df[g_m].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use 'Link Score' to filter out matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches with score > 0.5 : 284\n"
     ]
    }
   ],
   "source": [
    "match_mask = chk_df['Link Score'] >= 0.5\n",
    "chk_df_filtered = chk_df[match_mask]\n",
    "chk_df_filtered.to_csv(OUT_DIR + '/data_matching_output_FILTERED_FINAL.csv',index=False)\n",
    "print(f'Total matches with score > 0.5 : {chk_df_filtered.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cluster ID</th>\n",
       "      <th>Link Score</th>\n",
       "      <th>source file</th>\n",
       "      <th>fileName</th>\n",
       "      <th>id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.928825</td>\n",
       "      <td>0</td>\n",
       "      <td>pubmedf__890</td>\n",
       "      <td>890</td>\n",
       "      <td>risk of syncopal recurrences in patients treat...</td>\n",
       "      <td>bradyarrhythmic aetiology bradyarrhythmias tre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.928825</td>\n",
       "      <td>1</td>\n",
       "      <td>clinictrialf__237</td>\n",
       "      <td>237</td>\n",
       "      <td>risk of syncopal relapses in patients treated ...</td>\n",
       "      <td>bradyarrhythmic syncope underwent treated risk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1933</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.910573</td>\n",
       "      <td>1</td>\n",
       "      <td>clinictrialf__932</td>\n",
       "      <td>932</td>\n",
       "      <td>tricuspid regurgitation study</td>\n",
       "      <td>higher provide give septum months enroll leads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.910573</td>\n",
       "      <td>0</td>\n",
       "      <td>pubmedf__736</td>\n",
       "      <td>736</td>\n",
       "      <td>tricuspid regurgitation and implantable devices</td>\n",
       "      <td>multiphase defibrillator model pacemaker aorti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.875985</td>\n",
       "      <td>0</td>\n",
       "      <td>pubmedf__574</td>\n",
       "      <td>574</td>\n",
       "      <td>incidence of pacing-induced cardiomyopathy in ...</td>\n",
       "      <td>frequent underwent pacemaker picm ablation pat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.875985</td>\n",
       "      <td>1</td>\n",
       "      <td>clinictrialf__236</td>\n",
       "      <td>236</td>\n",
       "      <td>risk of pacing-induced cardiomyopathy</td>\n",
       "      <td>valvular may enhanced aortic cardiac conducted...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Cluster ID  Link Score  source file           fileName   id  \\\n",
       "890          0.0    0.928825            0       pubmedf__890  890   \n",
       "1238         0.0    0.928825            1  clinictrialf__237  237   \n",
       "1933         1.0    0.910573            1  clinictrialf__932  932   \n",
       "736          1.0    0.910573            0       pubmedf__736  736   \n",
       "574          2.0    0.875985            0       pubmedf__574  574   \n",
       "1237         2.0    0.875985            1  clinictrialf__236  236   \n",
       "\n",
       "                                                  Title  \\\n",
       "890   risk of syncopal recurrences in patients treat...   \n",
       "1238  risk of syncopal relapses in patients treated ...   \n",
       "1933                      tricuspid regurgitation study   \n",
       "736     tricuspid regurgitation and implantable devices   \n",
       "574   incidence of pacing-induced cardiomyopathy in ...   \n",
       "1237              risk of pacing-induced cardiomyopathy   \n",
       "\n",
       "                                               Keywords  \n",
       "890   bradyarrhythmic aetiology bradyarrhythmias tre...  \n",
       "1238  bradyarrhythmic syncope underwent treated risk...  \n",
       "1933  higher provide give septum months enroll leads...  \n",
       "736   multiphase defibrillator model pacemaker aorti...  \n",
       "574   frequent underwent pacemaker picm ablation pat...  \n",
       "1237  valvular may enhanced aortic cardiac conducted...  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chk_df_filtered.loc[~chk_df_filtered['Cluster ID'].isna()].sort_values(by=['Cluster ID']).head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_count(s):\n",
    "    return len([v for v in s.unique()])\n",
    "def agg_list(s):\n",
    "    return [v for v in s.unique()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Matched output with mapped ids Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_df = chk_df_filtered.groupby(by = ['Cluster ID'])['id'].agg([agg_list]).reset_index()\n",
    "dump_df['pubMed'] = np.array(dump_df.agg_list.to_list())[:,0]\n",
    "dump_df['clinicTrial'] = np.array(dump_df.agg_list.to_list())[:,1]\n",
    "dump_df.loc[:,['pubMed','clinicTrial']].to_csv(OUT_DIR + '/data_matching_Project_2_columnsOnly.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No common keywords found for 0 \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pubMed</th>\n",
       "      <th>clinicTrial</th>\n",
       "      <th>pubMed_Title</th>\n",
       "      <th>pubMed_keywords</th>\n",
       "      <th>clinicTrial_Title</th>\n",
       "      <th>clinicTrial_keywords</th>\n",
       "      <th>keywords_common</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>890</td>\n",
       "      <td>237</td>\n",
       "      <td>risk of syncopal recurrences in patients treat...</td>\n",
       "      <td>bradyarrhythmic aetiology bradyarrhythmias tre...</td>\n",
       "      <td>risk of syncopal relapses in patients treated ...</td>\n",
       "      <td>bradyarrhythmic syncope underwent treated risk...</td>\n",
       "      <td>pacemaker syncope permanent bradyarrhythmic im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>736</td>\n",
       "      <td>932</td>\n",
       "      <td>tricuspid regurgitation and implantable devices</td>\n",
       "      <td>multiphase defibrillator model pacemaker aorti...</td>\n",
       "      <td>tricuspid regurgitation study</td>\n",
       "      <td>higher provide give septum months enroll leads...</td>\n",
       "      <td>pacemaker cardiac aortic implantation regurgit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>574</td>\n",
       "      <td>236</td>\n",
       "      <td>incidence of pacing-induced cardiomyopathy in ...</td>\n",
       "      <td>frequent underwent pacemaker picm ablation pat...</td>\n",
       "      <td>risk of pacing-induced cardiomyopathy</td>\n",
       "      <td>valvular may enhanced aortic cardiac conducted...</td>\n",
       "      <td>pacemaker year (≥ using %) cardiomyopathy risk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>153</td>\n",
       "      <td>1</td>\n",
       "      <td>placement of cardiac pacemaker trial (pocket) ...</td>\n",
       "      <td>system pacemaker cardiac consecutive enroll si...</td>\n",
       "      <td>placement of cardiac pacemaker trial (pocket)-rct</td>\n",
       "      <td>defibrillator pacemaker 691 totally child ). v...</td>\n",
       "      <td>trial pacemaker cardiac rct two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>894</td>\n",
       "      <td>835</td>\n",
       "      <td>positron emission tomography in infective endo...</td>\n",
       "      <td>pacemaker cardiac lethal cases suspicion 5 val...</td>\n",
       "      <td>positron emission tomography of infection and ...</td>\n",
       "      <td>defibrillator bacteremia inflammatory interven...</td>\n",
       "      <td>pacemaker cardiac emission endocarditis tomogr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pubMed  clinicTrial                                       pubMed_Title  \\\n",
       "0     890          237  risk of syncopal recurrences in patients treat...   \n",
       "1     736          932    tricuspid regurgitation and implantable devices   \n",
       "2     574          236  incidence of pacing-induced cardiomyopathy in ...   \n",
       "3     153            1  placement of cardiac pacemaker trial (pocket) ...   \n",
       "4     894          835  positron emission tomography in infective endo...   \n",
       "\n",
       "                                     pubMed_keywords  \\\n",
       "0  bradyarrhythmic aetiology bradyarrhythmias tre...   \n",
       "1  multiphase defibrillator model pacemaker aorti...   \n",
       "2  frequent underwent pacemaker picm ablation pat...   \n",
       "3  system pacemaker cardiac consecutive enroll si...   \n",
       "4  pacemaker cardiac lethal cases suspicion 5 val...   \n",
       "\n",
       "                                   clinicTrial_Title  \\\n",
       "0  risk of syncopal relapses in patients treated ...   \n",
       "1                      tricuspid regurgitation study   \n",
       "2              risk of pacing-induced cardiomyopathy   \n",
       "3  placement of cardiac pacemaker trial (pocket)-rct   \n",
       "4  positron emission tomography of infection and ...   \n",
       "\n",
       "                                clinicTrial_keywords  \\\n",
       "0  bradyarrhythmic syncope underwent treated risk...   \n",
       "1  higher provide give septum months enroll leads...   \n",
       "2  valvular may enhanced aortic cardiac conducted...   \n",
       "3  defibrillator pacemaker 691 totally child ). v...   \n",
       "4  defibrillator bacteremia inflammatory interven...   \n",
       "\n",
       "                                     keywords_common  \n",
       "0  pacemaker syncope permanent bradyarrhythmic im...  \n",
       "1  pacemaker cardiac aortic implantation regurgit...  \n",
       "2  pacemaker year (≥ using %) cardiomyopathy risk...  \n",
       "3                    trial pacemaker cardiac rct two  \n",
       "4  pacemaker cardiac emission endocarditis tomogr...  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def join_keywords(k1,k2):\n",
    "    s1 = set(k1.split(' '))\n",
    "    s2 = set(k2.split(' '))\n",
    "    return ' '.join(list(s1.intersection(s2)))\n",
    "\n",
    "merge_CnP_df = dump_df.loc[:,['pubMed','clinicTrial']]\n",
    "out_merge_CnP_df = (merge_CnP_df\n",
    " .merge(pubMed_df,left_on=['pubMed'],right_on=['id']).rename(columns={'id':'pubMed_id','Title':'pubMed_Title','Keywords':'pubMed_keywords'})\n",
    " .merge(clinic_df,left_on=['clinicTrial'],right_on=['id']).rename(columns={'id':'clinicTrial_id','Title':'clinicTrial_Title','Keywords':'clinicTrial_keywords'})\n",
    " .assign(keywords_common = lambda df : df.apply(lambda x: join_keywords(x.pubMed_keywords, x.clinicTrial_keywords), axis=1) )\n",
    " .drop(['pubMed_id','clinicTrial_id'],axis=1)\n",
    ")\n",
    "\n",
    "print(f' No common keywords found for {out_merge_CnP_df.keywords_common.isna().sum()} ')\n",
    "out_merge_CnP_df.to_csv(OUT_DIR + '/data_matching_Project_IMP_columnsOnly.csv')\n",
    "out_merge_CnP_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample output to check matched entitties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['risk of syncopal recurrences in patients treated with permanent pacing for bradyarrhythmic syncope: role of correlation between symptoms and electrocardiogram findings',\n",
       "        'risk of syncopal relapses in patients treated with permanent pacing for bradyarrhythmic syncope'],\n",
       "       ['tricuspid regurgitation and implantable devices',\n",
       "        'tricuspid regurgitation study']], dtype='<U239')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump_dfT=chk_df.groupby(by = ['Cluster ID'])['Title'].agg([agg_list]).reset_index()\n",
    "np.array(dump_dfT.agg_list.to_list())[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We need to create a ground-truth table with few enteries to verify that dedupe is matching as expected\n",
    "#### Read ground-Truth Table to check prec/recall only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnd_truth_df = pd.read_csv(OUT_DIR + '/gndTable_ProjectF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = pd.read_csv(OUT_DIR + '/data_matching_Project_2_columnsOnly.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_count(s):\n",
    "    return len([v for v in s.unique()])\n",
    "def agg_list(s):\n",
    "    return [v for v in s.unique()]\n",
    "\n",
    "def evaluateDuplicates(found_dupes, true_dupes):\n",
    "    true_positives = found_dupes.intersection(true_dupes)\n",
    "    false_positives = found_dupes.difference(true_dupes)\n",
    "    uncovered_dupes = true_dupes.difference(found_dupes)\n",
    "\n",
    "    print('found duplicate')\n",
    "    print(len(found_dupes))\n",
    "    \n",
    "    print(f'true_positives : {len(true_positives)}, false_positives : {len(false_positives)}, true_dupes : {len(true_dupes)}')\n",
    "\n",
    "    prec = len(true_positives) / float(len(found_dupes))\n",
    "    print(f'precision : {prec}')    \n",
    "\n",
    "    rec = len(true_positives) / float(len(true_dupes))\n",
    "    print(f'recall : {rec}')\n",
    "        \n",
    "    F1 = 2 * (prec * rec) / (prec + rec)\n",
    "    print(f'F1 score : {F1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dupes = set(map(tuple, np.array(gnd_truth_df).tolist()))\n",
    "found_dupes = set(map(tuple, np.array(out_df).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found duplicate\n",
      "142\n",
      "true_positives : 24, false_positives : 118, true_dupes : 24\n",
      "precision : 0.16901408450704225\n",
      "recall : 1.0\n",
      "F1 score : 0.2891566265060241\n"
     ]
    }
   ],
   "source": [
    "evaluateDuplicates(found_dupes,true_dupes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trick to create ground-truth Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnd_mask = chk_df_filtered['Link Score'] >= 0.7\n",
    "dump_df = chk_df_filtered[gnd_mask].groupby(by = ['Cluster ID'])['id'].agg([agg_list]).reset_index()\n",
    "dump_df['pubMed'] = np.array(dump_df.agg_list.to_list())[:,0]\n",
    "dump_df['clinicTrial'] = np.array(dump_df.agg_list.to_list())[:,1]\n",
    "dump_df.loc[:,['pubMed','clinicTrial']].to_csv(DATA_DIR + '/gndTable_ProjectF.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
